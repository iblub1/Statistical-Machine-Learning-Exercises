\newif\ifvimbug
\vimbugfalse

\ifvimbug
\begin{document}
\fi

\exercise{Non-parametric Density Estimation}
 
In this exercise, you will use the datasets \texttt{nonParamTrain.txt} for training and \texttt{nonParamTest.txt} for evaluating the performance of your model.

\begin{questions}

%----------------------------------------------

\begin{question}{Histogram}{4}
Compute and plot the histograms using $0.02$, $0.5$, $2.0$ size bins of the training data.  
Intuitively, indicate which bin size performs the best and explain why. Knowing only these three trials, would you be sure that the one you picked is truly the best one? Attach the plot of the histograms and a snippet of your code.

\begin{answer}
The first figure \ref{fig:histogram1} shows a Histogram of the data with bin size 0.02. The "spicky" behavior and empty bins which are conssistend throughout the Histogram, indicate that the bin size has been chosen to to small (to much Variance).	

In the next Figure \ref{fig:histogram2} you can imagine a trend for the underlying distribution. This indicates a good bin size.


The third histogram \ref{fig:histogram3} also schows a trend of the underlying function, but compared to figure \ref{fig:histogram2} some of details get lost in the representation. Meaning that the bin size has been choosen to big (to much Bias).

Bonus: This last histogram \ref{fig:histogram4} is an histogram with the number of bins determined by Sturges Rule. It's shape is similiar to figure (2), only beeing a bit more smooth. However Sturges rule is critized to smooth the Histogram to much (Hyndman, 1995) and should be only used as a rule of thumb. In this case it adds futher believe into a bin size of about 0.5

However in the end for these four bin sizes, we cant be completly sure that the one choose is the best, but we have atleast good evidence for it.
	
\end{answer}
\end{question}


\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.4\linewidth}
		\includegraphics[width=\linewidth]{pictures/histogram002.png}
		\caption{Bin Size 0.02.}
		\label{fig:histogram1}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\linewidth}
		\includegraphics[width=\linewidth]{pictures/histogram05.png}
		\caption{Bin Size 0.5.}
		\label{fig:histogram2}
	\end{subfigure}
	\caption{First two Histograms}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.4\linewidth}
		\includegraphics[width=\linewidth]{pictures/histogram2.png}
		\caption{Bin Size 2}
		\label{fig:histogram3}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\linewidth}
		\includegraphics[width=\linewidth]{pictures/histogramSturge.png}
		\caption{Bin size calculated with Sturges Rule.}
		\label{fig:histogram4}
	\end{subfigure}
	\caption{Last two Histograms}
	
\end{figure}

	
Snippet of the code used: 
\begin{lstlisting}{}
import numpy as np
train = np.loadtxt("nonParamTrain.txt");

import matplotlib.pyplot as plt
binwidth = 0.02
bin_sequence = np.arange(min(train), max(train) + binwidth, binwidth)  
plt.hist(train, bins=bin_sequence, edgecolor="black" ,linewidth=0.1);
\end{lstlisting}

%----------------------------------------------

\begin{question}{Kernel Density Estimate}{6}
Compute the probability density estimate using a Gaussian kernel with $\sigma=0.03$, $\sigma=0.2$ and $\sigma=0.8$ of the training data. Compute the log-likelihood of the data for each case, compare them and show which parameter performs the best.
Generate a single plot with the different density estimates and attach it to your solutions. Plot the densities in the interval $x \in [-4,8]$, attach a snippet of your code and comment the results.

\begin{answer}\end{answer}

\end{question}

%----------------------------------------------

\begin{question}{K-Nearest Neighbors}{6}
Estimate the probability density with the K-nearest neighbors method with $K=2, K=8, K=35$.
Generate a single plot with the different density estimates and attach it to your solutions. Plot the densities in the interval $x \in [-4,8]$, attach a snippet of your code and comment the results.

\begin{answer}\end{answer}

\end{question}

%----------------------------------------------

\begin{question}{Comparison of the Non-Parametric Methods}{4}
Estimate the log-likelihood of the testing data using the KDE estimators and the K-NN estimators.
Why do we need to test them on a different data set? Compare the log-likelihoods of the estimators w.r.t. both the training and testing sets in a table. Which estimator would you choose?

\begin{answer}\end{answer}

\end{question}

\end{questions}
