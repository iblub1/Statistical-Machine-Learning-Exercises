\newif\ifvimbug
\vimbugfalse

\ifvimbug
\begin{document}
\fi

\exercise{Bayesian Decision Theory}
In this exercise, we consider data generated by a mixture of two Gaussian distributions with parameters $\{\mu_1$, $\sigma_1\}$ and $\{\mu_2$, $\sigma_2\}$. Each Gaussian represents a class labeled $C_1$ and $C_2$, respectively. 

\begin{questions}

%----------------------------------------------

\begin{question}{Optimal Boundary}{4}
Explain in one short sentence what Bayesian Decision Theory is. What is its goal? 
What condition does hold at the optimal decision boundary? When do we decide for class $C_1$ over $C_2$?

\begin{answer}\end{answer}

\end{question}


%----------------------------------------------

\begin{question}{Decision Boundaries}{8}
If both classes have equal prior probabilities $p(C_1) = p(C_2)$ and the same variance $\sigma_1 = \sigma_2$, derive the decision boundary $x^*$ analytically as a function of the two means $\mu_1$ and $\mu_2$.

\begin{answer}\end{answer}

\end{question}

%----------------------------------------------

\begin{question}{Different Misclassification Costs}{8}
Assume $\mu_1 > 0$, $\mu_1 = 2\mu_2$, $\sigma_1=\sigma_2$ and $p(C_1) = p(C_2)$. If misclassifying sample $x \in C_2$ as class $C_1$ is three times more expensive than the opposite, how does the decision boundary change? Derive the boundary analytically.
(There is no cost for correctly classifying samples.)

\begin{answer}\end{answer}

\end{question}

%----------------------------------------------

\end{questions}
